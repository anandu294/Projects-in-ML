{"nbformat_minor": 2, "cells": [{"source": "import numpy as np\nimport pandas as p\nimport matplotlib.pyplot as plt\nimport sklearn.metrics as metrics\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 1}, {"source": "\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share your notebook.\nclient_9319f604cbd54e56a04bf0a189e39fa5 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='sZCwam6tshI-F2SNmxXNArionqvl0a2l_1THVgRa3FQ5',\n    ibm_auth_endpoint=\"https://iam.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n\nbody = client_9319f604cbd54e56a04bf0a189e39fa5.get_object(Bucket='liverpatientanalysis-donotdelete-pr-uzrg2s8tjf3vwt',Key='Indian Liver Patient Dataset (ILPD).csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf = pd.read_csv(body)\ndf.head()\n\n", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "   age  gender  tot_bilirubin  direct_bilirubin  tot_proteins  albumin  \\\n0   65  Female            0.7               0.1           187       16   \n1   62    Male           10.9               5.5           699       64   \n2   62    Male            7.3               4.1           490       60   \n3   58    Male            1.0               0.4           182       14   \n4   72    Male            3.9               2.0           195       27   \n\n   ag_ratio  sgpt  sgot  alkphos  is_patient  \n0        18   6.8   3.3     0.90           1  \n1       100   7.5   3.2     0.74           1  \n2        68   7.0   3.3     0.89           1  \n3        20   6.8   3.4     1.00           1  \n4        59   7.3   2.4     0.40           1  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>gender</th>\n      <th>tot_bilirubin</th>\n      <th>direct_bilirubin</th>\n      <th>tot_proteins</th>\n      <th>albumin</th>\n      <th>ag_ratio</th>\n      <th>sgpt</th>\n      <th>sgot</th>\n      <th>alkphos</th>\n      <th>is_patient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>65</td>\n      <td>Female</td>\n      <td>0.7</td>\n      <td>0.1</td>\n      <td>187</td>\n      <td>16</td>\n      <td>18</td>\n      <td>6.8</td>\n      <td>3.3</td>\n      <td>0.90</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>62</td>\n      <td>Male</td>\n      <td>10.9</td>\n      <td>5.5</td>\n      <td>699</td>\n      <td>64</td>\n      <td>100</td>\n      <td>7.5</td>\n      <td>3.2</td>\n      <td>0.74</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>62</td>\n      <td>Male</td>\n      <td>7.3</td>\n      <td>4.1</td>\n      <td>490</td>\n      <td>60</td>\n      <td>68</td>\n      <td>7.0</td>\n      <td>3.3</td>\n      <td>0.89</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>58</td>\n      <td>Male</td>\n      <td>1.0</td>\n      <td>0.4</td>\n      <td>182</td>\n      <td>14</td>\n      <td>20</td>\n      <td>6.8</td>\n      <td>3.4</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72</td>\n      <td>Male</td>\n      <td>3.9</td>\n      <td>2.0</td>\n      <td>195</td>\n      <td>27</td>\n      <td>59</td>\n      <td>7.3</td>\n      <td>2.4</td>\n      <td>0.40</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "execution_count": 2, "metadata": {}}], "execution_count": 2}, {"source": "df", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "     age  gender  tot_bilirubin  direct_bilirubin  tot_proteins  albumin  \\\n0     65  Female            0.7               0.1           187       16   \n1     62    Male           10.9               5.5           699       64   \n2     62    Male            7.3               4.1           490       60   \n3     58    Male            1.0               0.4           182       14   \n4     72    Male            3.9               2.0           195       27   \n5     46    Male            1.8               0.7           208       19   \n6     26  Female            0.9               0.2           154       16   \n7     29  Female            0.9               0.3           202       14   \n8     17    Male            0.9               0.3           202       22   \n9     55    Male            0.7               0.2           290       53   \n10    57    Male            0.6               0.1           210       51   \n11    72    Male            2.7               1.3           260       31   \n12    64    Male            0.9               0.3           310       61   \n13    74  Female            1.1               0.4           214       22   \n14    61    Male            0.7               0.2           145       53   \n15    25    Male            0.6               0.1           183       91   \n16    38    Male            1.8               0.8           342      168   \n17    33    Male            1.6               0.5           165       15   \n18    40  Female            0.9               0.3           293      232   \n19    40  Female            0.9               0.3           293      232   \n20    51    Male            2.2               1.0           610       17   \n21    51    Male            2.9               1.3           482       22   \n22    62    Male            6.8               3.0           542      116   \n23    40    Male            1.9               1.0           231       16   \n24    63    Male            0.9               0.2           194       52   \n25    34    Male            4.1               2.0           289      875   \n26    34    Male            4.1               2.0           289      875   \n27    34    Male            6.2               3.0           240     1680   \n28    20    Male            1.1               0.5           128       20   \n29    84  Female            0.7               0.2           188       13   \n..   ...     ...            ...               ...           ...      ...   \n553   46    Male           10.2               4.2           232       58   \n554   73    Male            1.8               0.9           220       20   \n555   55    Male            0.8               0.2           290      139   \n556   51    Male            0.7               0.1           180       25   \n557   51    Male            2.9               1.2           189       80   \n558   51    Male            4.0               2.5           275      382   \n559   26    Male           42.8              19.7           390       75   \n560   66    Male           15.2               7.7           356      321   \n561   66    Male           16.6               7.6           315      233   \n562   66    Male           17.3               8.5           388      173   \n563   64    Male            1.4               0.5           298       31   \n564   38  Female            0.6               0.1           165       22   \n565   43    Male           22.5              11.8           143       22   \n566   50  Female            1.0               0.3           191       22   \n567   52    Male            2.7               1.4           251       20   \n568   20  Female           16.7               8.4           200       91   \n569   16    Male            7.7               4.1           268      213   \n570   16    Male            2.6               1.2           236      131   \n571   90    Male            1.1               0.3           215       46   \n572   32    Male           15.6               9.5           134       54   \n573   32    Male            3.7               1.6           612       50   \n574   32    Male           12.1               6.0           515       48   \n575   32    Male           25.0              13.7           560       41   \n576   32    Male           15.0               8.2           289       58   \n577   32    Male           12.7               8.4           190       28   \n578   60    Male            0.5               0.1           500       20   \n579   40    Male            0.6               0.1            98       35   \n580   52    Male            0.8               0.2           245       48   \n581   31    Male            1.3               0.5           184       29   \n582   38    Male            1.0               0.3           216       21   \n\n     ag_ratio  sgpt  sgot  alkphos  is_patient  \n0          18   6.8   3.3     0.90           1  \n1         100   7.5   3.2     0.74           1  \n2          68   7.0   3.3     0.89           1  \n3          20   6.8   3.4     1.00           1  \n4          59   7.3   2.4     0.40           1  \n5          14   7.6   4.4     1.30           1  \n6          12   7.0   3.5     1.00           1  \n7          11   6.7   3.6     1.10           1  \n8          19   7.4   4.1     1.20           2  \n9          58   6.8   3.4     1.00           1  \n10         59   5.9   2.7     0.80           1  \n11         56   7.4   3.0     0.60           1  \n12         58   7.0   3.4     0.90           2  \n13         30   8.1   4.1     1.00           1  \n14         41   5.8   2.7     0.87           1  \n15         53   5.5   2.3     0.70           2  \n16        441   7.6   4.4     1.30           1  \n17         23   7.3   3.5     0.92           2  \n18        245   6.8   3.1     0.80           1  \n19        245   6.8   3.1     0.80           1  \n20         28   7.3   2.6     0.55           1  \n21         34   7.0   2.4     0.50           1  \n22         66   6.4   3.1     0.90           1  \n23         55   4.3   1.6     0.60           1  \n24         45   6.0   3.9     1.85           2  \n25        731   5.0   2.7     1.10           1  \n26        731   5.0   2.7     1.10           1  \n27        850   7.2   4.0     1.20           1  \n28         30   3.9   1.9     0.95           2  \n29         21   6.0   3.2     1.10           2  \n..        ...   ...   ...      ...         ...  \n553       140   7.0   2.7     0.60           1  \n554        43   6.5   3.0     0.80           1  \n555        87   7.0   3.0     0.70           1  \n556        27   6.1   3.1     1.00           1  \n557       125   6.2   3.1     1.00           1  \n558       330   7.5   4.0     1.10           1  \n559       138   7.5   2.6     0.50           1  \n560       562   6.5   2.2     0.40           1  \n561       384   6.9   2.0     0.40           1  \n562       367   7.8   2.6     0.50           1  \n563        83   7.2   2.6     0.50           1  \n564        34   5.9   2.9     0.90           2  \n565       143   6.6   2.1     0.46           1  \n566        31   7.8   4.0     1.00           2  \n567        40   6.0   1.7     0.39           1  \n568       101   6.9   3.5     1.02           1  \n569       168   7.1   4.0     1.20           1  \n570        90   5.4   2.6     0.90           1  \n571       134   6.9   3.0     0.70           1  \n572       125   5.6   4.0     2.50           1  \n573        88   6.2   1.9     0.40           1  \n574        92   6.6   2.4     0.50           1  \n575        88   7.9   2.5     2.50           1  \n576        80   5.3   2.2     0.70           1  \n577        47   5.4   2.6     0.90           1  \n578        34   5.9   1.6     0.37           2  \n579        31   6.0   3.2     1.10           1  \n580        49   6.4   3.2     1.00           1  \n581        32   6.8   3.4     1.00           1  \n582        24   7.3   4.4     1.50           2  \n\n[583 rows x 11 columns]", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>gender</th>\n      <th>tot_bilirubin</th>\n      <th>direct_bilirubin</th>\n      <th>tot_proteins</th>\n      <th>albumin</th>\n      <th>ag_ratio</th>\n      <th>sgpt</th>\n      <th>sgot</th>\n      <th>alkphos</th>\n      <th>is_patient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>65</td>\n      <td>Female</td>\n      <td>0.7</td>\n      <td>0.1</td>\n      <td>187</td>\n      <td>16</td>\n      <td>18</td>\n      <td>6.8</td>\n      <td>3.3</td>\n      <td>0.90</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>62</td>\n      <td>Male</td>\n      <td>10.9</td>\n      <td>5.5</td>\n      <td>699</td>\n      <td>64</td>\n      <td>100</td>\n      <td>7.5</td>\n      <td>3.2</td>\n      <td>0.74</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>62</td>\n      <td>Male</td>\n      <td>7.3</td>\n      <td>4.1</td>\n      <td>490</td>\n      <td>60</td>\n      <td>68</td>\n      <td>7.0</td>\n      <td>3.3</td>\n      <td>0.89</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>58</td>\n      <td>Male</td>\n      <td>1.0</td>\n      <td>0.4</td>\n      <td>182</td>\n      <td>14</td>\n      <td>20</td>\n      <td>6.8</td>\n      <td>3.4</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72</td>\n      <td>Male</td>\n      <td>3.9</td>\n      <td>2.0</td>\n      <td>195</td>\n      <td>27</td>\n      <td>59</td>\n      <td>7.3</td>\n      <td>2.4</td>\n      <td>0.40</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>46</td>\n      <td>Male</td>\n      <td>1.8</td>\n      <td>0.7</td>\n      <td>208</td>\n      <td>19</td>\n      <td>14</td>\n      <td>7.6</td>\n      <td>4.4</td>\n      <td>1.30</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>26</td>\n      <td>Female</td>\n      <td>0.9</td>\n      <td>0.2</td>\n      <td>154</td>\n      <td>16</td>\n      <td>12</td>\n      <td>7.0</td>\n      <td>3.5</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>29</td>\n      <td>Female</td>\n      <td>0.9</td>\n      <td>0.3</td>\n      <td>202</td>\n      <td>14</td>\n      <td>11</td>\n      <td>6.7</td>\n      <td>3.6</td>\n      <td>1.10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>17</td>\n      <td>Male</td>\n      <td>0.9</td>\n      <td>0.3</td>\n      <td>202</td>\n      <td>22</td>\n      <td>19</td>\n      <td>7.4</td>\n      <td>4.1</td>\n      <td>1.20</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>55</td>\n      <td>Male</td>\n      <td>0.7</td>\n      <td>0.2</td>\n      <td>290</td>\n      <td>53</td>\n      <td>58</td>\n      <td>6.8</td>\n      <td>3.4</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>57</td>\n      <td>Male</td>\n      <td>0.6</td>\n      <td>0.1</td>\n      <td>210</td>\n      <td>51</td>\n      <td>59</td>\n      <td>5.9</td>\n      <td>2.7</td>\n      <td>0.80</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>72</td>\n      <td>Male</td>\n      <td>2.7</td>\n      <td>1.3</td>\n      <td>260</td>\n      <td>31</td>\n      <td>56</td>\n      <td>7.4</td>\n      <td>3.0</td>\n      <td>0.60</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>64</td>\n      <td>Male</td>\n      <td>0.9</td>\n      <td>0.3</td>\n      <td>310</td>\n      <td>61</td>\n      <td>58</td>\n      <td>7.0</td>\n      <td>3.4</td>\n      <td>0.90</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>74</td>\n      <td>Female</td>\n      <td>1.1</td>\n      <td>0.4</td>\n      <td>214</td>\n      <td>22</td>\n      <td>30</td>\n      <td>8.1</td>\n      <td>4.1</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>61</td>\n      <td>Male</td>\n      <td>0.7</td>\n      <td>0.2</td>\n      <td>145</td>\n      <td>53</td>\n      <td>41</td>\n      <td>5.8</td>\n      <td>2.7</td>\n      <td>0.87</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>25</td>\n      <td>Male</td>\n      <td>0.6</td>\n      <td>0.1</td>\n      <td>183</td>\n      <td>91</td>\n      <td>53</td>\n      <td>5.5</td>\n      <td>2.3</td>\n      <td>0.70</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>38</td>\n      <td>Male</td>\n      <td>1.8</td>\n      <td>0.8</td>\n      <td>342</td>\n      <td>168</td>\n      <td>441</td>\n      <td>7.6</td>\n      <td>4.4</td>\n      <td>1.30</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>33</td>\n      <td>Male</td>\n      <td>1.6</td>\n      <td>0.5</td>\n      <td>165</td>\n      <td>15</td>\n      <td>23</td>\n      <td>7.3</td>\n      <td>3.5</td>\n      <td>0.92</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>40</td>\n      <td>Female</td>\n      <td>0.9</td>\n      <td>0.3</td>\n      <td>293</td>\n      <td>232</td>\n      <td>245</td>\n      <td>6.8</td>\n      <td>3.1</td>\n      <td>0.80</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>40</td>\n      <td>Female</td>\n      <td>0.9</td>\n      <td>0.3</td>\n      <td>293</td>\n      <td>232</td>\n      <td>245</td>\n      <td>6.8</td>\n      <td>3.1</td>\n      <td>0.80</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>51</td>\n      <td>Male</td>\n      <td>2.2</td>\n      <td>1.0</td>\n      <td>610</td>\n      <td>17</td>\n      <td>28</td>\n      <td>7.3</td>\n      <td>2.6</td>\n      <td>0.55</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>51</td>\n      <td>Male</td>\n      <td>2.9</td>\n      <td>1.3</td>\n      <td>482</td>\n      <td>22</td>\n      <td>34</td>\n      <td>7.0</td>\n      <td>2.4</td>\n      <td>0.50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>62</td>\n      <td>Male</td>\n      <td>6.8</td>\n      <td>3.0</td>\n      <td>542</td>\n      <td>116</td>\n      <td>66</td>\n      <td>6.4</td>\n      <td>3.1</td>\n      <td>0.90</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>40</td>\n      <td>Male</td>\n      <td>1.9</td>\n      <td>1.0</td>\n      <td>231</td>\n      <td>16</td>\n      <td>55</td>\n      <td>4.3</td>\n      <td>1.6</td>\n      <td>0.60</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>63</td>\n      <td>Male</td>\n      <td>0.9</td>\n      <td>0.2</td>\n      <td>194</td>\n      <td>52</td>\n      <td>45</td>\n      <td>6.0</td>\n      <td>3.9</td>\n      <td>1.85</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>34</td>\n      <td>Male</td>\n      <td>4.1</td>\n      <td>2.0</td>\n      <td>289</td>\n      <td>875</td>\n      <td>731</td>\n      <td>5.0</td>\n      <td>2.7</td>\n      <td>1.10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>34</td>\n      <td>Male</td>\n      <td>4.1</td>\n      <td>2.0</td>\n      <td>289</td>\n      <td>875</td>\n      <td>731</td>\n      <td>5.0</td>\n      <td>2.7</td>\n      <td>1.10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>34</td>\n      <td>Male</td>\n      <td>6.2</td>\n      <td>3.0</td>\n      <td>240</td>\n      <td>1680</td>\n      <td>850</td>\n      <td>7.2</td>\n      <td>4.0</td>\n      <td>1.20</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>20</td>\n      <td>Male</td>\n      <td>1.1</td>\n      <td>0.5</td>\n      <td>128</td>\n      <td>20</td>\n      <td>30</td>\n      <td>3.9</td>\n      <td>1.9</td>\n      <td>0.95</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>84</td>\n      <td>Female</td>\n      <td>0.7</td>\n      <td>0.2</td>\n      <td>188</td>\n      <td>13</td>\n      <td>21</td>\n      <td>6.0</td>\n      <td>3.2</td>\n      <td>1.10</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>553</th>\n      <td>46</td>\n      <td>Male</td>\n      <td>10.2</td>\n      <td>4.2</td>\n      <td>232</td>\n      <td>58</td>\n      <td>140</td>\n      <td>7.0</td>\n      <td>2.7</td>\n      <td>0.60</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>554</th>\n      <td>73</td>\n      <td>Male</td>\n      <td>1.8</td>\n      <td>0.9</td>\n      <td>220</td>\n      <td>20</td>\n      <td>43</td>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>0.80</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>555</th>\n      <td>55</td>\n      <td>Male</td>\n      <td>0.8</td>\n      <td>0.2</td>\n      <td>290</td>\n      <td>139</td>\n      <td>87</td>\n      <td>7.0</td>\n      <td>3.0</td>\n      <td>0.70</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>556</th>\n      <td>51</td>\n      <td>Male</td>\n      <td>0.7</td>\n      <td>0.1</td>\n      <td>180</td>\n      <td>25</td>\n      <td>27</td>\n      <td>6.1</td>\n      <td>3.1</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>557</th>\n      <td>51</td>\n      <td>Male</td>\n      <td>2.9</td>\n      <td>1.2</td>\n      <td>189</td>\n      <td>80</td>\n      <td>125</td>\n      <td>6.2</td>\n      <td>3.1</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>558</th>\n      <td>51</td>\n      <td>Male</td>\n      <td>4.0</td>\n      <td>2.5</td>\n      <td>275</td>\n      <td>382</td>\n      <td>330</td>\n      <td>7.5</td>\n      <td>4.0</td>\n      <td>1.10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>559</th>\n      <td>26</td>\n      <td>Male</td>\n      <td>42.8</td>\n      <td>19.7</td>\n      <td>390</td>\n      <td>75</td>\n      <td>138</td>\n      <td>7.5</td>\n      <td>2.6</td>\n      <td>0.50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>560</th>\n      <td>66</td>\n      <td>Male</td>\n      <td>15.2</td>\n      <td>7.7</td>\n      <td>356</td>\n      <td>321</td>\n      <td>562</td>\n      <td>6.5</td>\n      <td>2.2</td>\n      <td>0.40</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>561</th>\n      <td>66</td>\n      <td>Male</td>\n      <td>16.6</td>\n      <td>7.6</td>\n      <td>315</td>\n      <td>233</td>\n      <td>384</td>\n      <td>6.9</td>\n      <td>2.0</td>\n      <td>0.40</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>562</th>\n      <td>66</td>\n      <td>Male</td>\n      <td>17.3</td>\n      <td>8.5</td>\n      <td>388</td>\n      <td>173</td>\n      <td>367</td>\n      <td>7.8</td>\n      <td>2.6</td>\n      <td>0.50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>563</th>\n      <td>64</td>\n      <td>Male</td>\n      <td>1.4</td>\n      <td>0.5</td>\n      <td>298</td>\n      <td>31</td>\n      <td>83</td>\n      <td>7.2</td>\n      <td>2.6</td>\n      <td>0.50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>38</td>\n      <td>Female</td>\n      <td>0.6</td>\n      <td>0.1</td>\n      <td>165</td>\n      <td>22</td>\n      <td>34</td>\n      <td>5.9</td>\n      <td>2.9</td>\n      <td>0.90</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>43</td>\n      <td>Male</td>\n      <td>22.5</td>\n      <td>11.8</td>\n      <td>143</td>\n      <td>22</td>\n      <td>143</td>\n      <td>6.6</td>\n      <td>2.1</td>\n      <td>0.46</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>50</td>\n      <td>Female</td>\n      <td>1.0</td>\n      <td>0.3</td>\n      <td>191</td>\n      <td>22</td>\n      <td>31</td>\n      <td>7.8</td>\n      <td>4.0</td>\n      <td>1.00</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>52</td>\n      <td>Male</td>\n      <td>2.7</td>\n      <td>1.4</td>\n      <td>251</td>\n      <td>20</td>\n      <td>40</td>\n      <td>6.0</td>\n      <td>1.7</td>\n      <td>0.39</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>20</td>\n      <td>Female</td>\n      <td>16.7</td>\n      <td>8.4</td>\n      <td>200</td>\n      <td>91</td>\n      <td>101</td>\n      <td>6.9</td>\n      <td>3.5</td>\n      <td>1.02</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>569</th>\n      <td>16</td>\n      <td>Male</td>\n      <td>7.7</td>\n      <td>4.1</td>\n      <td>268</td>\n      <td>213</td>\n      <td>168</td>\n      <td>7.1</td>\n      <td>4.0</td>\n      <td>1.20</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>570</th>\n      <td>16</td>\n      <td>Male</td>\n      <td>2.6</td>\n      <td>1.2</td>\n      <td>236</td>\n      <td>131</td>\n      <td>90</td>\n      <td>5.4</td>\n      <td>2.6</td>\n      <td>0.90</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>571</th>\n      <td>90</td>\n      <td>Male</td>\n      <td>1.1</td>\n      <td>0.3</td>\n      <td>215</td>\n      <td>46</td>\n      <td>134</td>\n      <td>6.9</td>\n      <td>3.0</td>\n      <td>0.70</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>572</th>\n      <td>32</td>\n      <td>Male</td>\n      <td>15.6</td>\n      <td>9.5</td>\n      <td>134</td>\n      <td>54</td>\n      <td>125</td>\n      <td>5.6</td>\n      <td>4.0</td>\n      <td>2.50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>573</th>\n      <td>32</td>\n      <td>Male</td>\n      <td>3.7</td>\n      <td>1.6</td>\n      <td>612</td>\n      <td>50</td>\n      <td>88</td>\n      <td>6.2</td>\n      <td>1.9</td>\n      <td>0.40</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>574</th>\n      <td>32</td>\n      <td>Male</td>\n      <td>12.1</td>\n      <td>6.0</td>\n      <td>515</td>\n      <td>48</td>\n      <td>92</td>\n      <td>6.6</td>\n      <td>2.4</td>\n      <td>0.50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>575</th>\n      <td>32</td>\n      <td>Male</td>\n      <td>25.0</td>\n      <td>13.7</td>\n      <td>560</td>\n      <td>41</td>\n      <td>88</td>\n      <td>7.9</td>\n      <td>2.5</td>\n      <td>2.50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>576</th>\n      <td>32</td>\n      <td>Male</td>\n      <td>15.0</td>\n      <td>8.2</td>\n      <td>289</td>\n      <td>58</td>\n      <td>80</td>\n      <td>5.3</td>\n      <td>2.2</td>\n      <td>0.70</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>577</th>\n      <td>32</td>\n      <td>Male</td>\n      <td>12.7</td>\n      <td>8.4</td>\n      <td>190</td>\n      <td>28</td>\n      <td>47</td>\n      <td>5.4</td>\n      <td>2.6</td>\n      <td>0.90</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>578</th>\n      <td>60</td>\n      <td>Male</td>\n      <td>0.5</td>\n      <td>0.1</td>\n      <td>500</td>\n      <td>20</td>\n      <td>34</td>\n      <td>5.9</td>\n      <td>1.6</td>\n      <td>0.37</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>579</th>\n      <td>40</td>\n      <td>Male</td>\n      <td>0.6</td>\n      <td>0.1</td>\n      <td>98</td>\n      <td>35</td>\n      <td>31</td>\n      <td>6.0</td>\n      <td>3.2</td>\n      <td>1.10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>580</th>\n      <td>52</td>\n      <td>Male</td>\n      <td>0.8</td>\n      <td>0.2</td>\n      <td>245</td>\n      <td>48</td>\n      <td>49</td>\n      <td>6.4</td>\n      <td>3.2</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>581</th>\n      <td>31</td>\n      <td>Male</td>\n      <td>1.3</td>\n      <td>0.5</td>\n      <td>184</td>\n      <td>29</td>\n      <td>32</td>\n      <td>6.8</td>\n      <td>3.4</td>\n      <td>1.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>582</th>\n      <td>38</td>\n      <td>Male</td>\n      <td>1.0</td>\n      <td>0.3</td>\n      <td>216</td>\n      <td>21</td>\n      <td>24</td>\n      <td>7.3</td>\n      <td>4.4</td>\n      <td>1.50</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>583 rows \u00d7 11 columns</p>\n</div>"}, "execution_count": 3, "metadata": {}}], "execution_count": 3}, {"source": "df.isnull().any()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "age                 False\ngender              False\ntot_bilirubin       False\ndirect_bilirubin    False\ntot_proteins        False\nalbumin             False\nag_ratio            False\nsgpt                False\nsgot                False\nalkphos              True\nis_patient          False\ndtype: bool"}, "execution_count": 4, "metadata": {}}], "execution_count": 4}, {"source": "df.fillna(df.mean(),inplace=True)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 5}, {"source": "df.isnull().any()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "age                 False\ngender              False\ntot_bilirubin       False\ndirect_bilirubin    False\ntot_proteins        False\nalbumin             False\nag_ratio            False\nsgpt                False\nsgot                False\nalkphos             False\nis_patient          False\ndtype: bool"}, "execution_count": 6, "metadata": {}}], "execution_count": 6}, {"source": "x=df.iloc[:,:10].values", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 7}, {"source": "x", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[65, 'Female', 0.7, ..., 6.8, 3.3, 0.9],\n       [62, 'Male', 10.9, ..., 7.5, 3.2, 0.74],\n       [62, 'Male', 7.3, ..., 7.0, 3.3, 0.89],\n       ..., \n       [52, 'Male', 0.8, ..., 6.4, 3.2, 1.0],\n       [31, 'Male', 1.3, ..., 6.8, 3.4, 1.0],\n       [38, 'Male', 1.0, ..., 7.3, 4.4, 1.5]], dtype=object)"}, "execution_count": 8, "metadata": {}}], "execution_count": 8}, {"source": "y=df.iloc[:,10:].values", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 9}, {"source": "y", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [2],\n       [2],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [1],\n       [2],\n       [1],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [1],\n       [2],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [2],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [2],\n       [2],\n       [2],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [2],\n       [1],\n       [1],\n       [1],\n       [2]])"}, "execution_count": 10, "metadata": {}}], "execution_count": 10}, {"source": "from sklearn.preprocessing import LabelEncoder\nlb_x=LabelEncoder()\nlb_y=LabelEncoder()\nx[:,1]=lb_x.fit_transform(x[:,1])\ny[:,0]=lb_y.fit_transform(y[:,0])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 11}, {"source": "y", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1]])"}, "execution_count": 12, "metadata": {}}], "execution_count": 12}, {"source": "from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 13}, {"source": "from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nx_train=sc.fit_transform(x_train)\nx_test=sc.transform(x_test)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n  warnings.warn(msg, DataConversionWarning)\n"}], "execution_count": 14}, {"source": "from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=5, p=2)\nknn.fit(x_train,y_train)\ny_pred_knn=knn.predict(x_test)\nac_knn=accuracy_score(y_test,y_pred_knn)\nconfusion_matrix(y_test,y_pred_knn)\nfpr, tpr, threshold=metrics.roc_curve(y_test,y_pred_knn)\nroc_auc_knn=metrics.auc(fpr,tpr)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  app.launch_new_instance()\n"}], "execution_count": 15}, {"source": "from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(x_train,y_train)\ny_pred_lr=lr.predict(x_test)\nac_lr=accuracy_score(y_test,y_pred_lr)\nconfusion_matrix(y_test,y_pred_lr)\nfpr, tpr, threshold=metrics.roc_curve(y_test,y_pred_lr)\nroc_auc_lr=metrics.auc(fpr,tpr)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n"}], "execution_count": 16}, {"source": "from sklearn.svm import SVC\nsv=SVC(kernel=\"rbf\")\nsv.fit(x_train,y_train)\ny_pred_sv=sv.predict(x_test)\nac_sv=accuracy_score(y_test,y_pred_sv)\nconfusion_matrix(y_test,y_pred_sv)\nfpr, tpr, threshold=metrics.roc_curve(y_test,y_pred_sv)\nroc_auc_sv=metrics.auc(fpr,tpr)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n"}], "execution_count": 17}, {"source": "from sklearn.svm import SVC\nsvm=SVC(kernel=\"linear\")\nsvm.fit(x_train,y_train)\ny_pred_svm=svm.predict(x_test)\nac_svm=accuracy_score(y_test,y_pred_svm)\nconfusion_matrix(y_test,y_pred_svm)\nfpr, tpr, threshold=metrics.roc_curve(y_test,y_pred_svm)\nroc_auc_svm=metrics.auc(fpr,tpr)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n"}], "execution_count": 18}, {"source": "from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ny_pred_dt=dt.predict(x_test)\nac_dt=accuracy_score(y_test,y_pred_dt)\nconfusion_matrix(y_test,y_pred_dt)\nfpr, tpr, threshold=metrics.roc_curve(y_test,y_pred_dt)\nroc_auc_dt=metrics.auc(fpr,tpr)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 19}, {"source": "from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(n_estimators=1000)\nrfc.fit(x_train,y_train)\ny_pred_rfc=rfc.predict(x_test)\nac_rfc=accuracy_score(y_test,y_pred_rfc)\nconfusion_matrix(y_test,y_pred_rfc)\nfpr, tpr, threshold=metrics.roc_curve(y_test,y_pred_rfc)\nroc_auc_rfc=metrics.auc(fpr,tpr)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  app.launch_new_instance()\n"}], "execution_count": 20}, {"source": "x=[\"LR\",\"KNN\",\"Linear SVM\",\"DT\",\"Gaussian SVM\",\"RFC\"]\nroc_auc=[roc_auc_lr,roc_auc_knn,roc_auc_svm,roc_auc_dt,roc_auc_sv,roc_auc_rfc]", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 21}, {"source": "import matplotlib.pyplot as plt\nplt.title('AUC')\nplt.bar(x, roc_auc)\nplt.show()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "display_data", "data": {"image/png": "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\n", "text/plain": "<matplotlib.figure.Figure at 0x7f8e6d3ab518>"}, "metadata": {}}], "execution_count": 22}, {"source": "x=[\"LR\",\"KNN\",\"Linear SVM\",\"DT\",\"Gaussian SVM\",\"RF\"]\ny=[ac_lr,ac_knn,ac_svm,ac_dt,ac_sv,ac_rfc]\nplt.bar(x,y)\nplt.title(\"Accuracy\",color='r')\nplt.legend", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "<function matplotlib.pyplot.legend>"}, "execution_count": 23, "metadata": {}}, {"output_type": "display_data", "data": {"image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFkpJREFUeJzt3X20XXV95/H3hwTwiVKVq+MQIBkaWaY+oMY406kWi45BOwFbtclohVU12jHVio5idTEMdnWqFumDcVW0jGiFFB+mxk5arA+sahVN0KAmNBJSMVemGh6EsagY+M4fZ188ub3J3Tc5Nzf58X6tdVbO/u3f2fu7b/b93N/5nXP2SVUhSWrLEXNdgCRp9Ax3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXYef5GqS20mOnutSpEOV4a7DS7IQeBpQwIqDuN/5B21f0ggY7jrcvAS4Bng/cPZ9rckDSS4iuYnkDpLPkzywW/eLJF8g+T7JTpJzuvarSV42tI1zSD4/tFwkryK5Abiha/vjbht3klxL8rSh/vNIfpfkRpL/160/gWQtyUV7HEXyCZLfGeUPRhpmuOtw8xLgQ93t2SSP7Nr/EHgy8AvAw4A3APeSnAj8DfCnwBhwKrB5Bvs7C3gqsKRb3tht42HA5cCHSR7QrTsXWAU8B/gZ4DeBu4DLgFUkg9+35DjgdOCKGdQhzYjhrsNH8ovAScCVVF0L3Aj8ly40fxN4DVXfoeoeqr5A1Y+BFwGfouoKqn5C1a1UzSTc/ydVt1H1QwCq/qLbxm6qLgKOBk7p+r4MeAtV26gqqq7r+n4ZuINBoAOsBK6m6rsH9gOR9s5w1+HkbOCTVN3SLV/etR0HPIBB2E92wl7a+9q5x1LyOpLru6mf7wPHdvufbl+XAS/u7r8Y+OAB1CRNyxeJdHgYzJ+/EJhH8s9d69HAzwKPAn4EnAxcN+mRO4Fle9nqvwAPGlr+N1P0+ellUwfz629kMALfQtW9JLcDGdrXycA3ptjOXwDfIHkC8Bjgr/ZSkzQSjtx1uDgLuIfB3Pep3e0xwOcYzMNfCryT5N92L2z+h+6tkh8CnknyQpL5JA8nObXb5mbgV0keRPJzwEunqeEYYDewC5hPcj6DufUJ7wPeSrKYJCSPJ3k4AFXjDObrPwh89L5pHmmWGO46XJwN/C+qvk3VP993g3cxmFc/D/g6gwC9DXgbcARV32bwAufruvbNwBO6bV4M3A18l8G0yYemqeEqBi/OfhO4icGzheFpm3cCVwKfBO4E/hx44ND6y4DH4ZSMDoL4ZR3SQZI8ncH0zEKq7p3rctQ2R+7SwZAcCbwGeJ/BroPBcJdmW/IY4PsMXvj9ozmuRvcTTstIUoMcuUtSg+bsfe7HHXdcLVy4cK52L0mHpWuvvfaWqhqbrt+chfvChQvZtGnTXO1ekg5LSW7q089pGUlqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSgXuGeZHmSbUm2JzlvivUXJ9nc3b6ZwZcYSJLmyLTvc08yD1gLPAsYBzYmWV9VWyf6VNVrh/r/NvDEWahVktRTn5H7MmB7Ve2oqruBdcCZ++i/Cr/4V5LmVJ9PqB7Pnl9IMM7g2+D/lSQnAYuAz+xl/WpgNcCJJ544o0IlaToLz/s/c11CL9/6g+fO+j76jNwzRdveLiW5EvhIVd0z1cqquqSqllbV0rGxaS+NIEnaT31G7uMMvtV9wgLg5r30XQm86kCLkg5Fjgp1OOkzct8ILE6yKMlRDAJ8/eROSU4BHgp8cbQlSpJmatpwr6rdwBoGXw58PXBlVW1JcmGSFUNdVwHrym//kKQ51+uSv1W1Adgwqe38ScsXjK4sSdKB8BOqktQgw12SGmS4S1KDDHdJatCcfYeq9uR7qHWwec61zZG7JDXosBy5O+KQpH1z5C5JDTLcJalBh+W0jA59Tp1Jc8uRuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QG9Qr3JMuTbEuyPcl5e+nzwiRbk2xJcvloy5QkzcS015ZJMg9YCzwLGAc2JllfVVuH+iwG3gT8x6q6PckjZqtgSdL0+ozclwHbq2pHVd0NrAPOnNTn5cDaqrodoKq+N9oyJUkz0Sfcjwd2Di2Pd23DHg08Osk/JLkmyfJRFShJmrk+l/zNFG01xXYWA6cBC4DPJXlsVX1/jw0lq4HVACeeeOKMi5Uk9dNn5D4OnDC0vAC4eYo+H6+qn1TVPwHbGIT9HqrqkqpaWlVLx8bG9rdmSdI0+oT7RmBxkkVJjgJWAusn9fkr4BkASY5jME2zY5SFSpL6mzbcq2o3sAa4CrgeuLKqtiS5MMmKrttVwK1JtgKfBf5bVd06W0VLkvat19fsVdUGYMOktvOH7hdwbneTJM0xP6EqSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1KBe4Z5keZJtSbYnOW+K9eck2ZVkc3d72ehLlST1NX+6DknmAWuBZwHjwMYk66tq66Suf1lVa2ahRknSDPUZuS8DtlfVjqq6G1gHnDm7ZUmSDkSfcD8e2Dm0PN61TfZrSb6W5CNJTphqQ0lWJ9mUZNOuXbv2o1xJUh99wj1TtNWk5U8AC6vq8cCngMum2lBVXVJVS6tq6djY2MwqlST11ifcx4HhkfgC4ObhDlV1a1X9uFt8L/Dk0ZQnSdoffcJ9I7A4yaIkRwErgfXDHZI8amhxBXD96EqUJM3UtO+WqardSdYAVwHzgEurakuSC4FNVbUeeHWSFcBu4DbgnFmsWZI0jWnDHaCqNgAbJrWdP3T/TcCbRluaJGl/+QlVSWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1qFe4J1meZFuS7UnO20e/5yepJEtHV6IkaaamDfck84C1wBnAEmBVkiVT9DsGeDXwpVEXKUmamT4j92XA9qraUVV3A+uAM6fo91bg7cCPRlifJGk/9An344GdQ8vjXdt9kjwROKGq/nqEtUmS9lOfcM8UbXXfyuQI4GLgddNuKFmdZFOSTbt27epfpSRpRvqE+zhwwtDyAuDmoeVjgMcCVyf5FvDvgfVTvahaVZdU1dKqWjo2Nrb/VUuS9qlPuG8EFidZlOQoYCWwfmJlVd1RVcdV1cKqWghcA6yoqk2zUrEkaVrThntV7QbWAFcB1wNXVtWWJBcmWTHbBUqSZm5+n05VtQHYMKnt/L30Pe3Ay5IkHQg/oSpJDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUoF7hnmR5km1Jtic5b4r1r0zy9SSbk3w+yZLRlypJ6mvacE8yD1gLnAEsAVZNEd6XV9XjqupU4O3AO0deqSSptz4j92XA9qraUVV3A+uAM4c7VNWdQ4sPBmp0JUqSZmp+jz7HAzuHlseBp07ulORVwLnAUcAvT7WhJKuB1QAnnnjiTGuVJPXUZ+SeKdr+1ci8qtZW1cnAG4G3TLWhqrqkqpZW1dKxsbGZVSpJ6q1PuI8DJwwtLwBu3kf/dcBZB1KUJOnA9An3jcDiJIuSHAWsBNYPd0iyeGjxucANoytRkjRT0865V9XuJGuAq4B5wKVVtSXJhcCmqloPrEnyTOAnwO3A2bNZtCRp3/q8oEpVbQA2TGo7f+j+a0ZclyTpAPgJVUlqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QG9Qr3JMuTbEuyPcl5U6w/N8nWJF9L8ukkJ42+VElSX9OGe5J5wFrgDGAJsCrJkkndvgosrarHAx8B3j7qQiVJ/fUZuS8DtlfVjqq6G1gHnDncoao+W1V3dYvXAAtGW6YkaSb6hPvxwM6h5fGubW9eCvzNVCuSrE6yKcmmXbt29a9SkjQjfcI9U7TVlB2TFwNLgXdMtb6qLqmqpVW1dGxsrH+VkqQZmd+jzzhwwtDyAuDmyZ2SPBN4M/BLVfXj0ZQnSdoffUbuG4HFSRYlOQpYCawf7pDkicB7gBVV9b3RlylJmolpw72qdgNrgKuA64Erq2pLkguTrOi6vQN4CPDhJJuTrN/L5iRJB0GfaRmqagOwYVLb+UP3nzniuiRJB8BPqEpSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUG9wj3J8iTbkmxPct4U65+e5CtJdid5/ujLlCTNxLThnmQesBY4A1gCrEqyZFK3bwPnAJePukBJ0szN79FnGbC9qnYAJFkHnAlsnehQVd/q1t07CzVKkmaoz7TM8cDOoeXxrm3GkqxOsinJpl27du3PJiRJPfQJ90zRVvuzs6q6pKqWVtXSsbGx/dmEJKmHPuE+DpwwtLwAuHl2ypEkjUKfcN8ILE6yKMlRwEpg/eyWJUk6ENOGe1XtBtYAVwHXA1dW1ZYkFyZZAZDkKUnGgRcA70myZTaLliTtW593y1BVG4ANk9rOH7q/kcF0jSTpEOAnVCWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIa1CvckyxPsi3J9iTnTbH+6CR/2a3/UpKFoy5UktTftOGeZB6wFjgDWAKsSrJkUreXArdX1c8BFwNvG3WhkqT++ozclwHbq2pHVd0NrAPOnNTnTOCy7v5HgNOTZHRlSpJmIlW17w7J84HlVfWybvk3gKdW1ZqhPt/o+ox3yzd2fW6ZtK3VwOpu8RRg26gOZASOA26ZttfhpbVjau14oL1jau144NA7ppOqamy6TvN7bGiqEfjkvwh9+lBVlwCX9NjnQZdkU1Utnes6Rqm1Y2rteKC9Y2rteODwPaY+0zLjwAlDywuAm/fWJ8l84FjgtlEUKEmauT7hvhFYnGRRkqOAlcD6SX3WA2d3958PfKamm++RJM2aaadlqmp3kjXAVcA84NKq2pLkQmBTVa0H/hz4YJLtDEbsK2ez6FlySE4XHaDWjqm144H2jqm144HD9JimfUFVknT48ROqktQgw12SGnS/C/ck9yTZnGRLkuuSnJvkiCTP7to3J/lBd7mFzUk+MMPtPzLJ5Ul2JLk2yReTPG+2jmdov0uT/MmItvUrSb7a/Xy2JnlFktOSfHFSv/lJvpvkUUnen+SuJMcMrf/jJJXkuGn294Oh+89JckOSE5Nc0G3zEXvpW0kuGlp+fZILDvDwZ81w7UNtFyT5TneubU2y6iDt95VJXjLqfU1Tx0E9r/ajvols+EaSTyT52a59YZIfDuXD5u7NJYe2qrpf3YAfDN1/BPAp4H9M6nM1sHQ/th3gi8Arh9pOAn57ro97BsdwJIO3ui7olo9m8IGzI4CdwMKhvsuBT3f33w98DXhxt3xEtzwOHNfn/wQ4HbgROLlbvgD4NvC2vfz//Qj4p4ntA68HLpjrn2Gfc2+o7QLg9d39xcCdwJGzvd+DcKwBjpjL8+pAfk4MPnH/5u7+QuAbc33+zPR2vxu5D6uq7zH4xOyaEV0u4ZeBu6vqz4b2cVNV/SncNwL4XJKvdLdf6NpPS/LXE49J8q4k53T3/6Ab5XwtyR92bS/oRhfXJfn7ydtIsizJF7pR0heSnNK1n5PkY0n+thsdv32KYziGwbuobu3q/3FVbauqe4EPA78+1HclcMXQ8hVD608D/gHY3ecHl+RpwHuB51bVjUOrLgV+PcnDpnjYbgbvZHhtn30c6qrqBuAu4KGzva/uGcPru/tXJ3lbki8n+Wb3f0GSeUnekWRjd/69omt/SJJPd+fw15Oc2bUvTHJ9kncDX2HPz8fMyXl1AL4IHD/L+5hV9+twB6iqHQx+Do+Yrm8PP8/gpN6b7wHPqqonMThZ9zmN0gXa84Cfr6rHA7/XrTofeHZVPQFYMcVD/xF4elU9sev7+0PrTu32/TgGoTn8C0hV3cbgcws3JbkiyYuSTJwnV9C9zTXJ0cBzgI8OPfwGYCzJQ4FVDK5D1MfRwMeBs6rqHyet+wGDgH/NXh67FnhRkmN77uuQleRJwA3doONgm19Vy4DfAf571/ZS4I6qegrwFODlSRYxeMb0vO48fgZw0dDg6BTgA1X1xKq6aWLjc3Re7ZcMLpZ4Ont+nufkoSmZtbO5/1G534d7Z1YucpZkbTe63tg1HQm8N8nXGYxWJl9dc7I7GfwivS/JrzIY1cFg5PL+JC9n8NmDyY4FPpzBNX8uZvBHZ8Knq+qOqvoRsJXBtNEeanAdodOBLzOY6ri0a98IPKR7JnAGcE1V3T7p4R9j8Iv6VOBz0xzfhJ8AX2AQJlP5E+DsJD8zRa13Ah8AXt1zX4ei1ybZBnyJwTTNXPhY9++1DKYhAP4T8JIkmxnU9nAGU0cBfj/J1xhMax4PPLJ7zE1Vdc1UO5iD82qmHtgd663Aw4C/G1p3Y1Wd2t1eNUv7H6n7fbgn+XfAPQxG1QdqC/CkiYXuJDgdmLjIz2uB7wJPAJYCEy/K7GbP/4sHdI/fzeCqnB8FzgL+tmt/JfAWBk97Nyd5+KQ63gp8tqoeC/znie11fjx0/x728kG2qvp6VV0MPAv4taFV6xj8kk1+6jy8/q3A33VPufu4F3gh8JQkvztFLd8HLgf+614e/0cM/jA8uOf+DjUXV9UpDJ5RfSDJA6Z7wCyYOC+Gz4kweL1oItQWVdUngRcxOKefXFWnMjinJ2r+l33t5CCfVzP1w+54TmLwu3lYhPje3K/DPckY8GfAu6p75eQAfQZ4QJLfGmp70ND9Y4H/252cv8FPR903AUsy+NKTYxn8QSDJQ4Bjq2oDg6fLp3btJ1fVl6rqfAZXq9tjaqXbz3e6++fM5AC6+dTThppO7eqbcAXwYgavL0y+DAVV9W3gzcC7Z7LfqroL+BUGUyxTjeDfCbyCKf4YdU/5r2TvI//DQlV9DNjETy/lMdeuAn4ryZEASR6d5MEMzq/vVdVPkjyDKZ79TTZX59X+qKo7GDwTfP3EsR+O+lwVsjUTT72OZDBi/iCD4DhgVVVJzgIuTvIGYBeDkcwbuy7vBj6a5AXAZ7t1VNXOJFcyeBfADcBXu/7HAB/vRnLhpy8cviPJxNPjTwPXAb80VMrbgcuSnMvgD85MBHhDkvcAP+xqPGfoGLcmuQu4tqqmHKVV1XtmuM+Jx92WZDnw90lumbTuliT/m72/eHoRsGYv6w4VD0oyPrQ81Xl3IXB5kveOcITaZ79TeR+DKZqvdHPquxg8g/wQ8Ikkm4DNDF7jmc6cnVf7o6q+muQ6Bs8kZmsaaFZ5+QFJatD9elpGklpluEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QG/X/Hu9Wycc99hQAAAABJRU5ErkJggg==\n", "text/plain": "<matplotlib.figure.Figure at 0x7f8e6d3ab4e0>"}, "metadata": {}}], "execution_count": 23}, {"source": "!pip install watson-machine-learning-client --upgrade", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Requirement already up-to-date: watson-machine-learning-client in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (1.0.365)\nRequirement not upgraded as not directly required: certifi in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client) (2019.3.9)\nRequirement not upgraded as not directly required: requests in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client) (2.18.4)\nRequirement not upgraded as not directly required: pandas in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client) (0.21.0)\nRequirement not upgraded as not directly required: ibm-cos-sdk in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client) (2.0.1)\nRequirement not upgraded as not directly required: urllib3 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client) (1.22)\nRequirement not upgraded as not directly required: lomond in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client) (0.1.13)\nRequirement not upgraded as not directly required: tabulate in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client) (0.8.2)\nRequirement not upgraded as not directly required: tqdm in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-machine-learning-client) (4.19.5)\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests->watson-machine-learning-client) (3.0.4)\nRequirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests->watson-machine-learning-client) (2.6)\nRequirement not upgraded as not directly required: python-dateutil>=2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pandas->watson-machine-learning-client) (2.6.1)\nRequirement not upgraded as not directly required: pytz>=2011k in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pandas->watson-machine-learning-client) (2018.3)\nRequirement not upgraded as not directly required: numpy>=1.9.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pandas->watson-machine-learning-client) (1.13.3)\nRequirement not upgraded as not directly required: ibm-cos-sdk-core==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk->watson-machine-learning-client) (2.0.1)\nRequirement not upgraded as not directly required: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk->watson-machine-learning-client) (2.0.1)\nRequirement not upgraded as not directly required: six>=1.10.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from lomond->watson-machine-learning-client) (1.11.0)\nRequirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk->watson-machine-learning-client) (0.9.3)\nRequirement not upgraded as not directly required: docutils>=0.10 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk->watson-machine-learning-client) (0.14)\n\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\n"}], "execution_count": 25}, {"source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n2019-05-23 05:03:07,620 - watson_machine_learning_client.metanames - WARNING - 'AUTHOR_EMAIL' meta prop is deprecated. It will be ignored.\n2019-05-23 05:03:54,339 - watson_machine_learning_client.metanames - WARNING - 'AUTHOR_EMAIL' meta prop is deprecated. It will be ignored.\n"}], "execution_count": 26}, {"source": "wml_credentials={\n  \"access_key\":\"88Hu_GnbmTKalcKSy6Z6poJqWke8TkC9_Q8ARbnHkzhH\",\n  \"instance_id\":\"f012e427-0872-4b79-b98a-71e39e402311\",\n  \"password\":\"de55ef26-f4d4-4f93-9a1e-d4fae1fbd610\",\n  \"url\":\"https://eu-gb.ml.cloud.ibm.com\",\n  \"username\":\"0aadb817-71c9-4efa-b747-5c8c429a19a8\"\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 27}, {"source": "client = WatsonMachineLearningAPIClient(wml_credentials)\nimport json", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 28}, {"source": "instance_details = client.service_instance.get_details()\nprint(json.dumps(instance_details, indent=2))", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "{\n  \"entity\": {\n    \"plan_id\": \"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe\",\n    \"organization_guid\": \"N/A\",\n    \"published_models\": {\n      \"url\": \"https://eu-gb.ml.cloud.ibm.com/v3/wml_instances/f012e427-0872-4b79-b98a-71e39e402311/published_models\"\n    },\n    \"account\": {\n      \"type\": \"STANDARD\",\n      \"id\": \"bfe1cab5e3414176901c055dc4aa8246\",\n      \"name\": \"Medikonda Naga sekhar's Account\"\n    },\n    \"owner\": {\n      \"email\": \"medikondanagasekharreddy@gmail.com\",\n      \"ibm_id\": \"5500034QJQ\",\n      \"beta_user\": false,\n      \"user_id\": \"834f04b1-c336-4ae9-abd5-0b5f91897f56\",\n      \"country_code\": \"IND\"\n    },\n    \"region\": \"eu-gb\",\n    \"space_guid\": \"N/A\",\n    \"status\": \"Active\",\n    \"deployments\": {\n      \"url\": \"https://eu-gb.ml.cloud.ibm.com/v3/wml_instances/f012e427-0872-4b79-b98a-71e39e402311/deployments\"\n    },\n    \"plan\": \"lite\",\n    \"tags\": null,\n    \"usage\": {\n      \"gpu_count_p100\": {\n        \"limit\": 0,\n        \"current\": 0\n      },\n      \"computation_time\": {\n        \"limit\": 180000,\n        \"current\": 0\n      },\n      \"deployment_count\": {\n        \"limit\": 5,\n        \"current\": 0\n      },\n      \"model_count\": {\n        \"limit\": 200,\n        \"current\": 1\n      },\n      \"capacity_units\": {\n        \"limit\": 180000000,\n        \"current\": 2\n      },\n      \"gpu_count_k80\": {\n        \"limit\": 8,\n        \"current\": 0\n      },\n      \"prediction_count\": {\n        \"limit\": 5000,\n        \"current\": 2\n      },\n      \"expiration_date\": \"2019-06-01T00:00:00.000Z\",\n      \"gpu_count_v100\": {\n        \"limit\": 0,\n        \"current\": 0\n      }\n    },\n    \"source\": \"Bluemix\"\n  },\n  \"metadata\": {\n    \"created_at\": \"2019-05-18T04:18:34.450Z\",\n    \"modified_at\": \"2019-05-18T04:18:34.450Z\",\n    \"guid\": \"f012e427-0872-4b79-b98a-71e39e402311\",\n    \"url\": \"https://eu-gb.ml.cloud.ibm.com/v3/wml_instances/f012e427-0872-4b79-b98a-71e39e402311\"\n  }\n}\n"}], "execution_count": 29}, {"source": "model_props = {client.repository.ModelMetaNames.AUTHOR_NAME: \"Naga sekhar\", \n               client.repository.ModelMetaNames.AUTHOR_EMAIL: \"medikondanagasekharreddy@gmail.com\", \n               client.repository.ModelMetaNames.NAME: \"Random Forest\"}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 30}, {"source": "model_artifact =client.repository.store_model(rfc, meta_props=model_props)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 33}, {"source": "published_model_uid = client.repository.get_model_uid(model_artifact)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 34}, {"source": "published_model_uid", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "'a0c6f888-5551-4a6a-a2e4-dd06b12f3c50'"}, "execution_count": 35, "metadata": {}}], "execution_count": 35}, {"source": "created_deployment = client.deployments.create(published_model_uid, name=\"Liver patient Analysis\")", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: 'a0c6f888-5551-4a6a-a2e4-dd06b12f3c50' started\n\n#######################################################################################\n\n\nINITIALIZING\nDEPLOY_SUCCESS\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='ff1d6289-a89e-4602-a17b-67c1b9ed4131'\n------------------------------------------------------------------------------------------------\n\n\n"}], "execution_count": 37}, {"source": "scoring_endpoint = client.deployments.get_scoring_url(created_deployment)\nscoring_endpoint", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "'https://eu-gb.ml.cloud.ibm.com/v3/wml_instances/f012e427-0872-4b79-b98a-71e39e402311/deployments/ff1d6289-a89e-4602-a17b-67c1b9ed4131/online'"}, "execution_count": 38, "metadata": {}}], "execution_count": 38}, {"source": "client.deployments.list()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "------------------------------------  ----------------------  ------  --------------  ------------------------  -----------------  -------------\nGUID                                  NAME                    TYPE    STATE           CREATED                   FRAMEWORK          ARTIFACT TYPE\nff1d6289-a89e-4602-a17b-67c1b9ed4131  Liver patient Analysis  online  DEPLOY_SUCCESS  2019-05-23T05:06:18.857Z  scikit-learn-0.19  model\n7ffcf530-a988-449b-9bca-0cc53ede2123  multilinear             online  DEPLOY_SUCCESS  2019-05-23T05:04:49.203Z  scikit-learn-0.19  model\ne9ffbd20-e2c8-4441-b13f-9bbf957e6972  multilinear             online  DEPLOY_SUCCESS  2019-05-18T05:13:05.184Z  scikit-learn-0.19  model\n------------------------------------  ----------------------  ------  --------------  ------------------------  -----------------  -------------\n"}], "execution_count": 39}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.5", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}